from flask import Flask, request, jsonify
import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFacePipeline
from utils import (
    custom_prompt_processing,
    extract_answer,
    handle_missing_env_vars,
    load_model_tokenizer_pipeline,
    load_documents_and_split,
    custom_retriever,
)

app = Flask(__name__)


@app.route("/query", methods=["POST"])
def handle_query():
    """
    Handle a POST request to the /query endpoint. It processes the query, retrieves relevant documents,
    and returns the answer generated by the language model along with document sources.

    :return: A JSON response containing the answer and sources.
    """

    data = request.json
    query = data.get("query")

    if not query:
        return jsonify({"error": "Query parameter is required"}), 400

    results = custom_retriever(query, vectordb)

    formatted_context = "\n\n".join(
        doc.page_content.replace("\n", " ") for doc in results
    )
    formatted_query = prompt.format(context=formatted_context, question=query)
    response = llm.invoke(formatted_query)

    answer_only = extract_answer(response)

    return jsonify(
        {
            "answer": answer_only,
            "sources": [
                {
                    "source": idx + 1,
                    "page": doc.metadata["page"],
                    "score": 1 - doc.metadata["score"],
                    "content": doc.page_content.replace("\n", " ")[:100] + "...",
                }
                for idx, doc in enumerate(results)
            ],
        }
    )


if __name__ == "__main__":

    # Load the HF_API_TOKEN from .env file
    load_dotenv()

    # Safety check for the API token
    HF_TOKEN = os.getenv("HF_API_TOKEN")
    handle_missing_env_vars(HF_TOKEN, "HF_API_TOKEN")

    # Load the model, tokenizer, and the pipeline
    tokenizer, model, query_pipeline = load_model_tokenizer_pipeline(
        model="meta-llama/Meta-Llama-3-8B-Instruct", token=HF_TOKEN
    )

    # Load PDF document and split it into smaller chunks and load into vector store
    vectordb = load_documents_and_split("data/NVIDIAAn.pdf", chunk_size=1000)

    # Define the custom prompt template
    prompt = custom_prompt_processing(
        template_str="""
        You are an assistant for question-answering tasks. Use the following context related to a company named NVIDIA to answer the question. 
        If you don't know the answer, just say that you don't know. Keep the answer concise and don't expand if the question is not related to the context.
        Do not include anything but the answer in the response.

        Question: {question}
        Context: {context}
        Answer:
        """
    )

    llm = HuggingFacePipeline(pipeline=query_pipeline)

    app.run(debug=True, port=2026)
